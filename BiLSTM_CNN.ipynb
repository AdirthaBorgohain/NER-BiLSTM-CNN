{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BWQ2h3gOVBkf"
   },
   "outputs": [],
   "source": [
    "# Importing all necessary python libraries and packages\n",
    "import joblib               # Loads and saves a serializable python object\n",
    "import numpy as np          # Performs mathemtical operations with arrays and matrices\n",
    "import pandas as pd         # Loads data to dataframe after reading data files\n",
    "from tqdm import tqdm       # Visualizes iterating through an iterable object\n",
    "from time import time       # Used to keep track of time elapsed\n",
    "\n",
    "# All tensorflow modules and layers are imported below\n",
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, Bidirectional, MaxPooling1D, \\\n",
    "    Flatten, concatenate\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.optimizers import Nadam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcbS0njJfzK6"
   },
   "source": [
    "We first specify some of the common parameters that we will use throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ceYoPUk-UT67"
   },
   "outputs": [],
   "source": [
    "MAX_CHAR_LEN = 52           # Maximum length of words that we consider. 52 has been choosen after analyzing the words in the dataset\n",
    "GloVe_emb_dim = 50          # Dimension of GloVe embeddings that we use in our model (More about gloVe embeddings: https://nlp.stanford.edu/projects/glove/)\n",
    "glove_embeddings_filepath = \"./embeddings/glove.6B.50d.txt\"     # Path where the gloVe embeddings file is kept\n",
    "\n",
    "# Paths of the train, test and validation files are listed below\n",
    "train_file = './data/train.txt'\n",
    "test_file = './data/test.txt'\n",
    "val_file = './data/valid.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StABYo-ZhPW3"
   },
   "source": [
    "# **Data Loading and Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3LTJEx4f7qC"
   },
   "source": [
    "We load the train, test and val data into separate dataframes first. For loading data into dataframes, we use the pandas library. (For more info: https://pandas.pydata.org/). Each word of a sentence is stored in a row alongwith the characters that make up the word and the tag associated with the word. The sentence to which a word belongs to can be deduced by looking at the 'sentence #' column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "CbUzaM6RVNek",
    "outputId": "a6d0ac63-7ba0-42fa-96f6-b284adf747e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence #</th>\n",
       "      <th>word</th>\n",
       "      <th>chars</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EU</td>\n",
       "      <td>[E, U]</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rejects</td>\n",
       "      <td>[r, e, j, e, c, t, s]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>German</td>\n",
       "      <td>[G, e, r, m, a, n]</td>\n",
       "      <td>B-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>call</td>\n",
       "      <td>[c, a, l, l]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>[t, o]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203616</th>\n",
       "      <td>14040</td>\n",
       "      <td>three</td>\n",
       "      <td>[t, h, r, e, e]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203617</th>\n",
       "      <td>14041</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>[S, w, a, n, s, e, a]</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203618</th>\n",
       "      <td>14041</td>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203619</th>\n",
       "      <td>14041</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[L, i, n, c, o, l, n]</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203620</th>\n",
       "      <td>14041</td>\n",
       "      <td>2</td>\n",
       "      <td>[2]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203621 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence #     word                  chars     tag\n",
       "0                1       EU                 [E, U]   B-ORG\n",
       "1                1  rejects  [r, e, j, e, c, t, s]       O\n",
       "2                1   German     [G, e, r, m, a, n]  B-MISC\n",
       "3                1     call           [c, a, l, l]       O\n",
       "4                1       to                 [t, o]       O\n",
       "...            ...      ...                    ...     ...\n",
       "203616       14040    three        [t, h, r, e, e]       O\n",
       "203617       14041  Swansea  [S, w, a, n, s, e, a]   B-ORG\n",
       "203618       14041        1                    [1]       O\n",
       "203619       14041  Lincoln  [L, i, n, c, o, l, n]   B-ORG\n",
       "203620       14041        2                    [2]       O\n",
       "\n",
       "[203621 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function that takes the path of a file in conLL format and loads them into a \n",
    "# dataframe.\n",
    "def load_data(filename):\n",
    "    ner_data, counter, sentence_counter = {}, 0, 1\n",
    "\n",
    "    with open(filename, mode='r') as f:\n",
    "        sentence_flag = False\n",
    "        for line in f:\n",
    "            if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
    "                if sentence_flag:\n",
    "                    sentence_flag = False\n",
    "                    sentence_counter += 1\n",
    "                continue\n",
    "            splits = line.split(' ')\n",
    "            sentence_flag = True\n",
    "            ner_data[counter] = {\"sentence #\": sentence_counter, \n",
    "                                 \"word\": splits[0], \n",
    "                                 \"chars\": [c for c in splits[0]], \n",
    "                                 \"tag\": splits[-1].strip()}\n",
    "            counter += 1\n",
    "            \n",
    "    if sentence_flag:\n",
    "        sentence_flag = False\n",
    "        sentence_counter += 1\n",
    "    return pd.DataFrame.from_dict(ner_data, \"index\")\n",
    "\n",
    "\n",
    "train_data = load_data(train_file)\n",
    "test_data = load_data(test_file)\n",
    "val_data = load_data(val_file)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apWHDOSin5HM"
   },
   "source": [
    "We use a bidirectional LSTM layer in our model that forms the core of the network and takes in the following three features as input:\n",
    "\n",
    "*   **Character-level** inputs from characters that make up a word and from which patterns are identified by a convolutional neural network\n",
    "\n",
    "*   **Word-level** input from GloVE embeddings wherein each word that occurs in the dataset are mapped to their gloVe embeddings.\n",
    "\n",
    "* **Casing** inputs that denotes whether words are lower case, upper case, etc. This helps the model in identifying appropriate entities by observing the casing that makes up a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFwazgeIgrQp"
   },
   "source": [
    "Next we extract out all unique words, tags and characters that are present in the given dataset. Each unique term is then mapped to a unique integer index and is stored as python dictionaries. These dictionaries help us encode the texts and characters to numeric values which a machine learning model accepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NO1UodIfVZuv",
    "outputId": "822caf02-6617-4f77-9e2d-e286e0729c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words:  26869\n",
      "Total number of unique tags:  9\n",
      "tag2idx:  {'I-LOC': 0, 'I-PER': 1, 'B-MISC': 2, 'I-MISC': 3, 'B-PER': 4, 'I-ORG': 5, 'O': 6, 'B-ORG': 7, 'B-LOC': 8}\n",
      "Total unique characters in dataset:  85\n"
     ]
    }
   ],
   "source": [
    "# mapping for words\n",
    "words = set(train_data[\"word\"].to_list() + test_data[\"word\"].to_list() + val_data[\"word\"].to_list())\n",
    "\n",
    "words = set([w.lower() for w in words])\n",
    "n_words = len(words)\n",
    "print(\"Total number of unique words: \", n_words)\n",
    "\n",
    "\n",
    "# mapping for tags\n",
    "tags = set(train_data[\"tag\"].to_list() + test_data[\"tag\"].to_list() + val_data[\"tag\"].to_list())\n",
    "n_tags = len(tags)\n",
    "\n",
    "tag2idx = {}\n",
    "for i, tag in enumerate(tags):\n",
    "    tag2idx[tag] = i\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "\n",
    "print(\"Total number of unique tags: \", n_tags)\n",
    "print(\"tag2idx: \", tag2idx)\n",
    "\n",
    "\n",
    "# mapping for characters\n",
    "chars = []\n",
    "for data in [train_data['chars'].to_list() + test_data['chars'].to_list() + val_data['chars'].to_list()]:\n",
    "    for word_char in data:\n",
    "        chars.extend(word_char)\n",
    "        chars = list(set(chars))\n",
    "\n",
    "# We manually map \"PAD\", \"UNK\" and \" \" to the char2idx mapping dictionary to \n",
    "# handle pad values and unknown values which we might come across later\n",
    "char2idx = {c: i + 3 for i, c in enumerate(chars)}\n",
    "char2idx[\"PAD\"] = 0\n",
    "char2idx[\"UNK\"] = 1\n",
    "char2idx[\" \"] = 2\n",
    "print(\"Total unique characters in dataset: \", len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAUO4RiqjTJs"
   },
   "source": [
    "The SentenceGetter class groups \"word\", \"chars\" and \"tag\" column by their \n",
    "sentence #s. We generate this grouped data for each of our train, test and \n",
    "validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rsvfQDpRVb7K"
   },
   "outputs": [],
   "source": [
    "class SentenceSegregator(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"word\"].values.tolist(),\n",
    "                                                           s[\"chars\"].values.tolist(),\n",
    "                                                           s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "\n",
    "# Create SentenceSegregator objects using train, test and val data    \n",
    "segregator_train = SentenceSegregator(train_data)\n",
    "segregator_test = SentenceSegregator(test_data)\n",
    "segregator_val = SentenceSegregator(val_data)\n",
    "\n",
    "# Extract sentences for the three different types of data\n",
    "sentences_train = segregator_train.sentences\n",
    "sentences_test = segregator_test.sentences\n",
    "sentences_val = segregator_val.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f-Mejy2jxMX",
    "outputId": "55d3531a-86f7-4e0e-afb5-4debd5e0b8a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('EU', ['E', 'U'], 'B-ORG'),\n",
       "  ('rejects', ['r', 'e', 'j', 'e', 'c', 't', 's'], 'O'),\n",
       "  ('German', ['G', 'e', 'r', 'm', 'a', 'n'], 'B-MISC'),\n",
       "  ('call', ['c', 'a', 'l', 'l'], 'O'),\n",
       "  ('to', ['t', 'o'], 'O'),\n",
       "  ('boycott', ['b', 'o', 'y', 'c', 'o', 't', 't'], 'O'),\n",
       "  ('British', ['B', 'r', 'i', 't', 'i', 's', 'h'], 'B-MISC'),\n",
       "  ('lamb', ['l', 'a', 'm', 'b'], 'O'),\n",
       "  ('.', ['.'], 'O')],\n",
       " [('Peter', ['P', 'e', 't', 'e', 'r'], 'B-PER'),\n",
       "  ('Blackburn', ['B', 'l', 'a', 'c', 'k', 'b', 'u', 'r', 'n'], 'I-PER')],\n",
       " [('BRUSSELS', ['B', 'R', 'U', 'S', 'S', 'E', 'L', 'S'], 'B-LOC'),\n",
       "  ('1996-08-22', ['1', '9', '9', '6', '-', '0', '8', '-', '2', '2'], 'O')]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample view of how extracted data looks like\n",
    "sentences_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv5t-mqoj6FT"
   },
   "source": [
    "Next we will use the gloVe embeddings file to map each of the words that is present in our dataset to gloVe embeddings. word2idx dictionary stores a mapping from a word to an integer index. word_embeddings dictionary on the other hand, stores a mapping from its corresponding integer index from word2idx to its gloVe embedding. Each of the embedding that we use here are 50 dimensional. We use the embeddings here that are trained on 6 billion wikipedia articles. This data is very extensive and covers a wide range of domains and fields which is why it is appropriate to use this embeddings for our usecase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RkKZ4ChmVkLb",
    "outputId": "aeab794a-24ad-4543-f7bc-cecdc6c4d3b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating word embeddings dict...: 400000it [00:11, 34733.39it/s]\n"
     ]
    }
   ],
   "source": [
    "word2idx = {}\n",
    "word_embeddings = []\n",
    "\n",
    "# add \"PAD\" and \"UNK\" word tokens\n",
    "word2idx[\"PAD\"] = 0     # \"PAD\" word is mapped to index 0\n",
    "# zero vector is kept as embedding for 'PAD' word\n",
    "vector = np.zeros(GloVe_emb_dim)  \n",
    "word_embeddings.append(vector)\n",
    "\n",
    "word2idx[\"UNK\"] = 1     # \"UNK\" word is mapped to index 1\n",
    "# vector generated from a uniform distribution is kept as embedding for 'UNK' \n",
    "# word. This word is used for unknown words\n",
    "vector = np.random.uniform(-0.25, 0.25, GloVe_emb_dim) \n",
    "word_embeddings.append(vector)\n",
    "\n",
    "# load gloVe word embeddings into word_embeddings dictionary while populating \n",
    "# word2idx dictionary at the same time\n",
    "with open(glove_embeddings_filepath, encoding=\"utf-8\") as fin:\n",
    "    # iterate through each word in gloVe embeddings file. tqdm is used for \n",
    "    # visualizing progress\n",
    "    for line in tqdm(fin, desc=\"generating word embeddings dict...\"):\n",
    "        split = line.strip().split(\" \")\n",
    "        word = split[0]  # embedding word entry\n",
    "\n",
    "        if split[0].lower() in words:\n",
    "            vector = np.array([float(num) for num in split[1:]])\n",
    "            word_embeddings.append(vector)  # word embedding vector\n",
    "            word2idx[split[0]] = len(word2idx)  # corresponding word dict\n",
    "\n",
    "word_embeddings = np.array(word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXSyccAWmFyg"
   },
   "source": [
    "Now using word2idx and word_embeddings dictionary, we can easily fetch the gloVe embedding for a word that occurs in our data. For example, let us consider the word 'california' To find out its embedding, we can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miQeZQrxnNu5",
    "outputId": "f2272c88-2454-4a8e-dc69-1c601dfea856"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.094576,  0.12842 , -0.099356,  0.46342 , -0.99674 , -0.041353,\n",
       "       -1.815   , -0.20399 ,  0.69405 ,  0.056446, -0.51808 , -0.73496 ,\n",
       "        0.97266 , -0.42734 ,  0.14013 , -0.5512  , -0.29905 ,  0.5736  ,\n",
       "       -0.33651 , -0.47473 , -0.23651 , -0.2431  , -0.56144 ,  0.2039  ,\n",
       "       -0.60267 , -1.9407  , -0.084779, -0.079509, -0.26599 , -0.59215 ,\n",
       "        2.596   , -0.84078 , -0.78666 , -1.2118  ,  0.34527 , -0.89909 ,\n",
       "       -0.80615 ,  0.030595,  1.3677  , -0.41649 , -1.2243  ,  0.33881 ,\n",
       "        0.38883 ,  0.076045, -0.54893 ,  0.52766 , -0.084278, -0.81387 ,\n",
       "       -0.19385 ,  0.84154 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings[word2idx['california']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXKeJE5VnP24"
   },
   "source": [
    "To make our model more robust and improve its efficiency while performing named entity recognition, we generate features from the casing of the words. Each word that occurs in the dataset are mapped to one of the following types:\n",
    "\n",
    "\n",
    "*   **NUMERIC**: Words wherein all the characters of the word are numeric (eg. 2050)\n",
    "*   **LOWER**: Words wherein all characters of the word are in lowercase (eg. happy)\n",
    "*   **UPPER**: Words wherein all characters of the word are in uppercase (eg. USA)\n",
    "*   **TITLE**: Words wherein the first character is in uppercase and rest in lowercase (eg. Stanford)\n",
    "\n",
    "*   **OTHER**: All words that do not belong to any of the other types. (eg. /{?)\n",
    "*   **MAJORITY_NUMERIC**: All words wherein majority of the characters are numeric (eg. S20)\n",
    "*   **CONTAINS_DIGIT**: All words where there are numeric characters and numeric characters are not in majority. (eg. Apollo13)\n",
    "*   **PAD**: Padding words\n",
    "\n",
    "Each casing type is represented by an integer index and is stored in the case2idx dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1TcyJghmVezF"
   },
   "outputs": [],
   "source": [
    "# mapping for word casing features\n",
    "case2idx = {'NUMERIC': 0, 'LOWER': 1, 'UPPER': 2, 'TITLE': 3, 'OTHER': 4, 'MAJORITY_NUMERIC': 5,\n",
    "            'CONTAINS_DIGIT': 6, 'PAD': 7}\n",
    "\n",
    "case_embeddings = np.identity(len(case2idx), dtype='float32')  # identity matrix used\n",
    "\n",
    "# function to generate casing features so that model can learn from different patterns in words\n",
    "def get_casing(word, case_idx_dict):\n",
    "    casing = 'OTHER'\n",
    "\n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "\n",
    "    digits_frac = numDigits / float(len(word))\n",
    "\n",
    "    if word.isdigit():  # Is a digit\n",
    "        casing = 'NUMERIC'\n",
    "    elif digits_frac > 0.5:\n",
    "        casing = 'MAJORITY_NUMERIC'\n",
    "    elif word.islower():  # All lower case\n",
    "        casing = 'LOWER'\n",
    "    elif word.isupper():  # All upper case\n",
    "        casing = 'UPPER'\n",
    "    elif word[0].isupper():  # is a title, initial char upper, then all lower\n",
    "        casing = 'TITLE'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'CONTAINS_DIGIT'\n",
    "\n",
    "    return case_idx_dict[casing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4yTYh3DrzQB"
   },
   "source": [
    "We add all the features that we generated (word, case, character features) together along with its corresponding NER tag. Each of the train, test and validation data is processed and dataset contating all features we need for training is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJfP1zd5Vkpi",
    "outputId": "6c9982dd-59eb-464d-8161-2bf3a502d6e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:04,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set, val_set = [], [], []\n",
    "\n",
    "# generates matrices for train, test and val data with 1 entry -> list of 4 elements: word indices, case indices, character indices, label indices\n",
    "for dataset, data in tqdm(zip([train_set, test_set, val_set], [sentences_train, sentences_test, sentences_val])):\n",
    "    word_count = 0\n",
    "    unk_word_count = 0\n",
    "    \n",
    "    for sentence in data:\n",
    "        word_indices, case_indices, char_indices, tag_indices = [], [], [], []\n",
    "\n",
    "        for word, char, tag in sentence:\n",
    "            word_count += 1\n",
    "            if word in word2idx:\n",
    "                word_idx = word2idx[word]\n",
    "            elif word.lower() in word2idx:\n",
    "                word_idx = word2idx[word.lower()]\n",
    "            else:\n",
    "                word_idx = word2idx[\"UNK\"]\n",
    "                unk_word_count += 1\n",
    "            char_idx = []\n",
    "            for x in char:\n",
    "                char_idx.append(char2idx[x])\n",
    "                \n",
    "            # Get the label and map to int\n",
    "            word_indices.append(word_idx)\n",
    "            case_indices.append(get_casing(word, case2idx))\n",
    "            char_indices.append(char_idx)\n",
    "            tag_indices.append(tag2idx[tag])\n",
    "\n",
    "        dataset.append([word_indices, case_indices, char_indices, tag_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bWZz2ARsVl5I"
   },
   "outputs": [],
   "source": [
    "# Each of the words are padded to MAX_CHAR_LEN length which we defined earlier in the notebook.\n",
    "# This is done so as to make the length of each word same before passing the data to the model.\n",
    "def padding(sentences):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentences[i][2] = pad_sequences(sentences[i][2], MAX_CHAR_LEN, padding='post')\n",
    "    return sentences\n",
    "\n",
    "train_set = padding(train_set)\n",
    "test_set = padding(test_set)\n",
    "val_set = padding(val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QX9FSp9ss4D7"
   },
   "source": [
    "Now that we have generated all necessary features and prepared our train, test, val sets for training, we will see what information each of the instance in our set encodes. Lets pick a random index 39 and see how its generated features look like when we decode the numeric values back to human readable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EdyPHJyWVsga",
    "outputId": "664853ea-568f-4326-c73d-2378d5103c55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('china', 'TITLE', ['C', 'h', 'i', 'n', 'a'], 'B-LOC')\n",
      "('has', 'LOWER', ['h', 'a', 's'], 'O')\n",
      "('said', 'LOWER', ['s', 'a', 'i', 'd'], 'O')\n",
      "('it', 'LOWER', ['i', 't'], 'O')\n",
      "('was', 'LOWER', ['w', 'a', 's'], 'O')\n",
      "('time', 'LOWER', ['t', 'i', 'm', 'e'], 'O')\n",
      "('for', 'LOWER', ['f', 'o', 'r'], 'O')\n",
      "('political', 'LOWER', ['p', 'o', 'l', 'i', 't', 'i', 'c', 'a', 'l'], 'O')\n",
      "('talks', 'LOWER', ['t', 'a', 'l', 'k', 's'], 'O')\n",
      "('with', 'LOWER', ['w', 'i', 't', 'h'], 'O')\n",
      "('taiwan', 'TITLE', ['T', 'a', 'i', 'w', 'a', 'n'], 'B-LOC')\n",
      "('and', 'LOWER', ['a', 'n', 'd'], 'O')\n",
      "('that', 'LOWER', ['t', 'h', 'a', 't'], 'O')\n",
      "('the', 'LOWER', ['t', 'h', 'e'], 'O')\n",
      "('rival', 'LOWER', ['r', 'i', 'v', 'a', 'l'], 'O')\n",
      "('island', 'LOWER', ['i', 's', 'l', 'a', 'n', 'd'], 'O')\n",
      "('should', 'LOWER', ['s', 'h', 'o', 'u', 'l', 'd'], 'O')\n",
      "('take', 'LOWER', ['t', 'a', 'k', 'e'], 'O')\n",
      "('practical', 'LOWER', ['p', 'r', 'a', 'c', 't', 'i', 'c', 'a', 'l'], 'O')\n",
      "('steps', 'LOWER', ['s', 't', 'e', 'p', 's'], 'O')\n",
      "('towards', 'LOWER', ['t', 'o', 'w', 'a', 'r', 'd', 's'], 'O')\n",
      "('that', 'LOWER', ['t', 'h', 'a', 't'], 'O')\n",
      "('goal', 'LOWER', ['g', 'o', 'a', 'l'], 'O')\n",
      "('.', 'OTHER', ['.'], 'O')\n"
     ]
    }
   ],
   "source": [
    "idx2case = {v: k for k,v in case2idx.items()}\n",
    "idx2char = {v: k for k,v in char2idx.items()}\n",
    "idx2word = {v: k for k,v in word2idx.items()}\n",
    "\n",
    "\n",
    "idx = 39\n",
    "for i, j, k, l in zip(train_set[idx][0], train_set[idx][1], train_set[idx][2], train_set[idx][3]):\n",
    "    print((idx2word[i], idx2case[j], [idx2char[c] for c in k if c != 0], idx2tag[l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oS5TXplFuCt6"
   },
   "source": [
    "We divide our training data in batches which speeds up the training process. Basically, we use bucketing to achieve a speed up in the model training. We consider the length of the sentences and accordingly prepare batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rlI80UQFVuEl"
   },
   "outputs": [],
   "source": [
    "# Function to generate batches of feature extracted data\n",
    "def generate_batches(data):\n",
    "    l = []\n",
    "    for i in data:\n",
    "        l.append(len(i[0]))\n",
    "    l = set(l)\n",
    "    batches, batch_len = [], []\n",
    "    z = 0\n",
    "    for i in l:\n",
    "        for batch in data:\n",
    "            if len(batch[0]) == i:\n",
    "                batches.append(batch)\n",
    "                z += 1\n",
    "        batch_len.append(z)\n",
    "    return batches, batch_len\n",
    "\n",
    "# Function that is used to iterate through the generate batches while training\n",
    "def iterate_minibatches(dataset, batch_len):\n",
    "    start = 0\n",
    "    for i in batch_len:\n",
    "        tokens = []\n",
    "        casing = []\n",
    "        char = []\n",
    "        labels = []\n",
    "        data = dataset[start:i]\n",
    "        start = i\n",
    "        for dt in data:\n",
    "            t, c, ch, l = dt\n",
    "            l = np.expand_dims(l, -1)\n",
    "            tokens.append(t)\n",
    "            casing.append(c)\n",
    "            char.append(ch)\n",
    "            labels.append(l)\n",
    "        \n",
    "        yield np.asarray(labels), np.asarray(tokens), np.asarray(casing), np.asarray(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FrK1764CVwSX"
   },
   "outputs": [],
   "source": [
    "# We generate batches for our train, test and val sets\n",
    "train_batch, train_batch_len = generate_batches(train_set)\n",
    "test_batch, test_batch_len = generate_batches(test_set)\n",
    "val_batch, val_batch_len = generate_batches(val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxoEMGELqo3f"
   },
   "source": [
    "# **Model Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AxjD_0Qv7AW"
   },
   "source": [
    "We define the parameters of the model as well as the architecture of the model. We use a RNN architecture here to train our NER model. Using CNN and Bidirectional LSTM layers that can capture information in both forward and backward direction, we build a model and train it for 35 epochs using the features we generated above from the dataset. We can see a general overview of how the model is built from the model summary that is printed below. We use the sparse_categorical_crossentropy loss function for training our model. For optimizer, we use NADAM which is a variation on the well known ADAM optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZ6Hgf_ZVx0g",
    "outputId": "e8a9d774-3168-4a2c-f9b4-07f89cb4ca29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-14 10:21:54.512191: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-05-14 10:21:54.512266: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ThinkPad-P15s\n",
      "2022-05-14 10:21:54.512282: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ThinkPad-P15s\n",
      "2022-05-14 10:21:54.512458: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.47.3\n",
      "2022-05-14 10:21:54.512507: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.47.3\n",
      "2022-05-14 10:21:54.512520: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 510.47.3\n",
      "2022-05-14 10:21:54.513093: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " character_input (InputLayer)   [(None, None, 52)]   0           []                               \n",
      "                                                                                                  \n",
      " Character_embedding (TimeDistr  (None, None, 52, 30  2640       ['character_input[0][0]']        \n",
      " ibuted)                        )                                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 52, 30  0           ['Character_embedding[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Convolution (TimeDistributed)  (None, None, 52, 30  2730        ['dropout[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Maxpool (TimeDistributed)      (None, None, 1, 30)  0           ['Convolution[0][0]']            \n",
      "                                                                                                  \n",
      " words_input (InputLayer)       [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " casing_input (InputLayer)      [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " flatten_layer (TimeDistributed  (None, None, 30)    0           ['Maxpool[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 50)     1147450     ['words_input[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, None, 8)      64          ['casing_input[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, None, 30)     0           ['flatten_layer[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, None, 88)     0           ['embedding_1[0][0]',            \n",
      "                                                                  'embedding_2[0][0]',            \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " BiLSTM_layer (Bidirectional)   (None, None, 400)    462400      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " softmax_layer (TimeDistributed  (None, None, 9)     3609        ['BiLSTM_layer[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,618,893\n",
      "Trainable params: 471,379\n",
      "Non-trainable params: 1,147,514\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model built...\n"
     ]
    }
   ],
   "source": [
    "# char-level input\n",
    "char_in = Input(shape=(None, 52,), name=\"character_input\")\n",
    "embed_char_out = TimeDistributed(\n",
    "    Embedding(len(char2idx), 30, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\"Character_embedding\")(\n",
    "    char_in)\n",
    "dropout_out = Dropout(0.5)(embed_char_out)\n",
    "\n",
    "# CNN for char-level input\n",
    "conv1d_out = TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same', activation='tanh', strides=1), name=\"Convolution\")(dropout_out)\n",
    "maxpool_out = TimeDistributed(MaxPooling1D(52), name=\"Maxpool\")(conv1d_out)\n",
    "char = TimeDistributed(Flatten(), name=\"flatten_layer\")(maxpool_out)\n",
    "char = Dropout(0.5)(char)\n",
    "\n",
    "\n",
    "# word-level input\n",
    "words_in = Input(shape=(None,), dtype='int32', name='words_input')\n",
    "words = Embedding(input_dim=word_embeddings.shape[0], output_dim=word_embeddings.shape[1], \n",
    "                  weights=[word_embeddings], trainable=False)(words_in)\n",
    "\n",
    "\n",
    "# case-info input\n",
    "casing_in = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing = Embedding(output_dim=case_embeddings.shape[1], input_dim=case_embeddings.shape[0], weights=[case_embeddings],\n",
    "                   trainable=False)(casing_in)\n",
    "\n",
    "\n",
    "# concat all input layers & pass it through BiLSTM layer\n",
    "x = concatenate([words, casing, char])\n",
    "x = Bidirectional(LSTM(200, return_sequences=True, \n",
    "                            dropout=0.5,                # dropout on input to each LSTM block\n",
    "                            recurrent_dropout=0.25      # dropout on recurrent input signal\n",
    "                           ), name=\"BiLSTM_layer\")(x)\n",
    "\n",
    "output = TimeDistributed(Dense(len(tag2idx), activation='softmax'), name=\"softmax_layer\")(x)\n",
    "\n",
    "# Build and compile the model\n",
    "model = Model(inputs=[words_in, casing_in, char_in], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Nadam())\n",
    "\n",
    "print(model.summary())\n",
    "print(\"Model built...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzD_WiYR4Oma"
   },
   "source": [
    "Now that our model is built and initialized, we write some utility functions that helps us during training, predicting and evaluating steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TH469rhFVzG5"
   },
   "outputs": [],
   "source": [
    "# Function to make predictions on a set of data. Returns the predicted labels\n",
    "# as well as the true labels in integer indexes\n",
    "def predict(dataset):\n",
    "    true_labels, pred_labels = [], []\n",
    "    for i, data in enumerate(dataset):\n",
    "        tokens, casing, char, labels = data\n",
    "        tokens = np.asarray([tokens])\n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pred = model.predict([tokens, casing, char], verbose=False)[0]\n",
    "        pred = pred.argmax(axis=-1)  # Predict the classes\n",
    "        true_labels.append(labels)\n",
    "        pred_labels.append(pred)\n",
    "    return pred_labels, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "p8RDF_SaV08I"
   },
   "outputs": [],
   "source": [
    "# Function to generate recall and precision scores from given true labels and\n",
    "# predicted labels\n",
    "def get_score(l1, l2):\n",
    "    correct_count, count = 0, 0\n",
    "    for sent_idx in range(len(l1)):\n",
    "        l1_, l2_ = l1[sent_idx], l2[sent_idx]\n",
    "        idx = 0\n",
    "        while idx < len(l1_):\n",
    "            if l1_[idx][0] == 'B':  # a new chunk starts\n",
    "                count += 1\n",
    "                if l1_[idx] == l2_[idx]:  # first prediction correct\n",
    "                    idx += 1\n",
    "                    correct_flag = True\n",
    "                    while idx < len(l1_) and l1_[idx][0] == 'I':  # scan entire chunk\n",
    "                        if l1_[idx] != l2_[idx]:\n",
    "                            correct_flag = False\n",
    "                        idx += 1\n",
    "                    if idx < len(l1_):\n",
    "                        if l2_[idx][0] == 'I':  # chunk in l2_ was longer\n",
    "                            correct_flag = False\n",
    "                    if correct_flag:\n",
    "                        correct_count += 1\n",
    "                else:\n",
    "                    idx += 1\n",
    "            else:\n",
    "                idx += 1\n",
    "    score = 0\n",
    "    if count > 0:\n",
    "        score = float(correct_count) / count\n",
    "    return score\n",
    "\n",
    "\n",
    "# Function to compute the f1-score, precision and recall for given true and predicted labels\n",
    "def evaluate(pred_labels, true_labels):\n",
    "    p, t = [], []\n",
    "\n",
    "    for sent in true_labels:\n",
    "        p.append([idx2tag[e] for e in sent])\n",
    "    \n",
    "    for sent in pred_labels:\n",
    "        t.append([idx2tag[e] for e in sent])\n",
    "    \n",
    "    prec = get_score(p, t)\n",
    "    rec = get_score(t, p)\n",
    "\n",
    "    f1 = 0\n",
    "    if (rec + prec) > 0:\n",
    "        f1 = 2.0 * prec * rec / (prec + rec);\n",
    "\n",
    "    return prec, rec, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsgJpHiTq1li"
   },
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wuwd8zK44wbY"
   },
   "source": [
    "Since everything is now prepared and generated, we can move ahead to training our model with the training set we generated. We train the model with data in batches and for 35 epochs while observing the f1 score, precision and recall values for the test set and val set. After training is completed, we save the model as a whole as well as the trained model weights. These are used later in the notebook to load the trained model without having to train the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WHruuEKs0pJs",
    "outputId": "1b7fdef9-4ebf-4c83-c11c-552d57c1bbe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Epoch 0/35\n",
      "TEST SET: f1_score: 0.2101\tprecision: 0.4581\trecall: 0.1363\n",
      "VALIDATION SET: f1_score: 0.2263\tprecision: 0.5314\trecall: 0.1437\n",
      "Time taken: 672.8609082698822 secs\n",
      "\n",
      "====================\n",
      "Epoch 1/35\n",
      "TEST SET: f1_score: 0.5222\tprecision: 0.5835\trecall: 0.4726\n",
      "VALIDATION SET: f1_score: 0.5592\tprecision: 0.6599\trecall: 0.4852\n",
      "Time taken: 561.6324527263641 secs\n",
      "\n",
      "====================\n",
      "Epoch 2/35\n",
      "TEST SET: f1_score: 0.618\tprecision: 0.6487\trecall: 0.5901\n",
      "VALIDATION SET: f1_score: 0.6449\tprecision: 0.7064\trecall: 0.5932\n",
      "Time taken: 556.3135664463043 secs\n",
      "\n",
      "====================\n",
      "Epoch 3/35\n",
      "TEST SET: f1_score: 0.7029\tprecision: 0.7173\trecall: 0.6891\n",
      "VALIDATION SET: f1_score: 0.7211\tprecision: 0.7512\trecall: 0.6932\n",
      "Time taken: 583.7018115520477 secs\n",
      "\n",
      "====================\n",
      "Epoch 4/35\n",
      "TEST SET: f1_score: 0.6941\tprecision: 0.6811\trecall: 0.7075\n",
      "VALIDATION SET: f1_score: 0.718\tprecision: 0.7251\trecall: 0.711\n",
      "Time taken: 516.9007081985474 secs\n",
      "\n",
      "====================\n",
      "Epoch 5/35\n",
      "TEST SET: f1_score: 0.7293\tprecision: 0.7465\trecall: 0.7128\n",
      "VALIDATION SET: f1_score: 0.7352\tprecision: 0.766\trecall: 0.7068\n",
      "Time taken: 519.2690484523773 secs\n",
      "\n",
      "====================\n",
      "Epoch 6/35\n",
      "TEST SET: f1_score: 0.7766\tprecision: 0.7766\trecall: 0.7766\n",
      "VALIDATION SET: f1_score: 0.7832\tprecision: 0.7958\trecall: 0.7711\n",
      "Time taken: 509.0105426311493 secs\n",
      "\n",
      "====================\n",
      "Epoch 7/35\n",
      "TEST SET: f1_score: 0.7729\tprecision: 0.7679\trecall: 0.778\n",
      "VALIDATION SET: f1_score: 0.7758\tprecision: 0.7842\trecall: 0.7676\n",
      "Time taken: 510.7301254272461 secs\n",
      "\n",
      "====================\n",
      "Epoch 8/35\n",
      "TEST SET: f1_score: 0.774\tprecision: 0.7562\trecall: 0.7927\n",
      "VALIDATION SET: f1_score: 0.7784\tprecision: 0.7731\trecall: 0.7837\n",
      "Time taken: 504.1189136505127 secs\n",
      "\n",
      "====================\n",
      "Epoch 9/35\n",
      "TEST SET: f1_score: 0.7901\tprecision: 0.7939\trecall: 0.7863\n",
      "VALIDATION SET: f1_score: 0.799\tprecision: 0.8116\trecall: 0.7868\n",
      "Time taken: 503.0915598869324 secs\n",
      "\n",
      "====================\n",
      "Epoch 10/35\n",
      "TEST SET: f1_score: 0.7899\tprecision: 0.7888\trecall: 0.7909\n",
      "VALIDATION SET: f1_score: 0.7962\tprecision: 0.8017\trecall: 0.7908\n",
      "Time taken: 501.2762682437897 secs\n",
      "\n",
      "====================\n",
      "Epoch 11/35\n",
      "TEST SET: f1_score: 0.8032\tprecision: 0.8036\trecall: 0.8028\n",
      "VALIDATION SET: f1_score: 0.8195\tprecision: 0.8284\trecall: 0.8108\n",
      "Time taken: 502.03165674209595 secs\n",
      "\n",
      "====================\n",
      "Epoch 12/35\n",
      "TEST SET: f1_score: 0.8085\tprecision: 0.8057\trecall: 0.8113\n",
      "VALIDATION SET: f1_score: 0.8238\tprecision: 0.8273\trecall: 0.8204\n",
      "Time taken: 495.7826476097107 secs\n",
      "\n",
      "====================\n",
      "Epoch 13/35\n",
      "TEST SET: f1_score: 0.7806\tprecision: 0.7712\trecall: 0.7902\n",
      "VALIDATION SET: f1_score: 0.7989\tprecision: 0.7951\trecall: 0.8028\n",
      "Time taken: 502.865695476532 secs\n",
      "\n",
      "====================\n",
      "Epoch 14/35\n",
      "TEST SET: f1_score: 0.8208\tprecision: 0.8209\trecall: 0.8206\n",
      "VALIDATION SET: f1_score: 0.8418\tprecision: 0.8464\trecall: 0.8373\n",
      "Time taken: 495.0561602115631 secs\n",
      "\n",
      "====================\n",
      "Epoch 15/35\n",
      "TEST SET: f1_score: 0.8232\tprecision: 0.8149\trecall: 0.8316\n",
      "VALIDATION SET: f1_score: 0.8339\tprecision: 0.8312\trecall: 0.8368\n",
      "Time taken: 497.27668356895447 secs\n",
      "\n",
      "====================\n",
      "Epoch 16/35\n",
      "TEST SET: f1_score: 0.8366\tprecision: 0.8375\trecall: 0.8357\n",
      "VALIDATION SET: f1_score: 0.8465\tprecision: 0.8498\trecall: 0.8433\n",
      "Time taken: 503.0236699581146 secs\n",
      "\n",
      "====================\n",
      "Epoch 17/35\n",
      "TEST SET: f1_score: 0.8275\tprecision: 0.8241\trecall: 0.8309\n",
      "VALIDATION SET: f1_score: 0.8428\tprecision: 0.8436\trecall: 0.8421\n",
      "Time taken: 504.5014729499817 secs\n",
      "\n",
      "====================\n",
      "Epoch 18/35\n",
      "TEST SET: f1_score: 0.8376\tprecision: 0.8312\trecall: 0.844\n",
      "VALIDATION SET: f1_score: 0.8544\tprecision: 0.8525\trecall: 0.8563\n",
      "Time taken: 506.802734375 secs\n",
      "\n",
      "====================\n",
      "Epoch 19/35\n",
      "TEST SET: f1_score: 0.8422\tprecision: 0.8387\trecall: 0.8458\n",
      "VALIDATION SET: f1_score: 0.8588\tprecision: 0.858\trecall: 0.8595\n",
      "Time taken: 510.068265914917 secs\n",
      "\n",
      "====================\n",
      "Epoch 20/35\n",
      "TEST SET: f1_score: 0.8435\tprecision: 0.8449\trecall: 0.8421\n",
      "VALIDATION SET: f1_score: 0.8583\tprecision: 0.8611\trecall: 0.8556\n",
      "Time taken: 508.603120803833 secs\n",
      "\n",
      "====================\n",
      "Epoch 21/35\n",
      "TEST SET: f1_score: 0.8415\tprecision: 0.8401\trecall: 0.843\n",
      "VALIDATION SET: f1_score: 0.8609\tprecision: 0.8629\trecall: 0.859\n",
      "Time taken: 510.2081301212311 secs\n",
      "\n",
      "====================\n",
      "Epoch 22/35\n",
      "TEST SET: f1_score: 0.8428\tprecision: 0.8361\trecall: 0.8497\n",
      "VALIDATION SET: f1_score: 0.8643\tprecision: 0.8599\trecall: 0.8687\n",
      "Time taken: 511.83845949172974 secs\n",
      "\n",
      "====================\n",
      "Epoch 23/35\n",
      "TEST SET: f1_score: 0.842\tprecision: 0.8431\trecall: 0.841\n",
      "VALIDATION SET: f1_score: 0.8621\tprecision: 0.8633\trecall: 0.8608\n",
      "Time taken: 514.256618976593 secs\n",
      "\n",
      "====================\n",
      "Epoch 24/35\n",
      "TEST SET: f1_score: 0.8494\tprecision: 0.8435\trecall: 0.8553\n",
      "VALIDATION SET: f1_score: 0.87\tprecision: 0.8654\trecall: 0.8746\n",
      "Time taken: 509.77412509918213 secs\n",
      "\n",
      "====================\n",
      "Epoch 25/35\n",
      "TEST SET: f1_score: 0.85\tprecision: 0.8487\trecall: 0.8513\n",
      "VALIDATION SET: f1_score: 0.8704\tprecision: 0.8693\trecall: 0.8716\n",
      "Time taken: 509.1103458404541 secs\n",
      "\n",
      "====================\n",
      "Epoch 26/35\n",
      "TEST SET: f1_score: 0.8488\tprecision: 0.8538\trecall: 0.8438\n",
      "VALIDATION SET: f1_score: 0.8665\tprecision: 0.8725\trecall: 0.8605\n",
      "Time taken: 505.53935146331787 secs\n",
      "\n",
      "====================\n",
      "Epoch 27/35\n",
      "TEST SET: f1_score: 0.8463\tprecision: 0.8407\trecall: 0.852\n",
      "VALIDATION SET: f1_score: 0.8607\tprecision: 0.8576\trecall: 0.8639\n",
      "Time taken: 630.9398262500763 secs\n",
      "\n",
      "====================\n",
      "Epoch 28/35\n",
      "TEST SET: f1_score: 0.8544\tprecision: 0.8492\trecall: 0.8596\n",
      "VALIDATION SET: f1_score: 0.8784\tprecision: 0.8757\trecall: 0.8812\n",
      "Time taken: 552.8267412185669 secs\n",
      "\n",
      "====================\n",
      "Epoch 29/35\n",
      "TEST SET: f1_score: 0.86\tprecision: 0.8606\trecall: 0.8594\n",
      "VALIDATION SET: f1_score: 0.885\tprecision: 0.8867\trecall: 0.8834\n",
      "Time taken: 629.2733747959137 secs\n",
      "\n",
      "====================\n",
      "Epoch 30/35\n",
      "TEST SET: f1_score: 0.8508\tprecision: 0.8501\trecall: 0.8515\n",
      "VALIDATION SET: f1_score: 0.8768\tprecision: 0.8772\trecall: 0.8765\n",
      "Time taken: 587.4337944984436 secs\n",
      "\n",
      "====================\n",
      "Epoch 31/35\n",
      "TEST SET: f1_score: 0.8512\tprecision: 0.8457\trecall: 0.8568\n",
      "VALIDATION SET: f1_score: 0.8731\tprecision: 0.8695\trecall: 0.8768\n",
      "Time taken: 522.0349729061127 secs\n",
      "\n",
      "====================\n",
      "Epoch 32/35\n",
      "TEST SET: f1_score: 0.861\tprecision: 0.8588\trecall: 0.8633\n",
      "VALIDATION SET: f1_score: 0.8863\tprecision: 0.8853\trecall: 0.8872\n",
      "Time taken: 564.2132713794708 secs\n",
      "\n",
      "====================\n",
      "Epoch 33/35\n",
      "TEST SET: f1_score: 0.8618\tprecision: 0.8625\trecall: 0.861\n",
      "VALIDATION SET: f1_score: 0.8935\tprecision: 0.8957\trecall: 0.8913\n",
      "Time taken: 617.8566787242889 secs\n",
      "\n",
      "====================\n",
      "Epoch 34/35\n",
      "TEST SET: f1_score: 0.8556\tprecision: 0.8519\trecall: 0.8594\n",
      "VALIDATION SET: f1_score: 0.8817\tprecision: 0.8796\trecall: 0.8839\n",
      "Time taken: 632.4093899726868 secs\n",
      "\n",
      "Final F1 test score:  0.8556319407720784\n",
      "Training complete.\n",
      "Model and model weights saved.\n"
     ]
    }
   ],
   "source": [
    "f1_test_history = []\n",
    "f1_val_history = []\n",
    "\n",
    "epochs = 35\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time()\n",
    "    print(\"=\" * 20)\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "    for i, batch in enumerate(iterate_minibatches(train_batch, train_batch_len)):\n",
    "        labels, tokens, casing, char = batch       \n",
    "        model.train_on_batch([tokens, casing, char], labels)\n",
    "    \n",
    "    # evaluate on test set and validation set\n",
    "    pred_labels, true_labels = predict(test_batch)\n",
    "    pre_test, rec_test, f1_test = evaluate(pred_labels, true_labels)\n",
    "    f1_test_history.append(f1_test)\n",
    "    print(f\"TEST SET: f1_score: {round(f1_test, 4)}\\tprecision: {round(pre_test, 4)}\\trecall: {round(rec_test, 4)}\")\n",
    "    \n",
    "    pred_labels, true_labels = predict(val_batch)\n",
    "    pre_val, rec_val, f1_val = evaluate(pred_labels, true_labels)\n",
    "    f1_val_history.append(f1_val)\n",
    "    print(f\"VALIDATION SET: f1_score: {round(f1_val, 4)}\\tprecision: {round(pre_val, 4)}\\trecall: {round(rec_val, 4)}\")\n",
    "    \n",
    "    print(f\"Time taken: {time() - start_time} secs\\n\")\n",
    "\n",
    "print(\"Final F1 test score: \", f1_test)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# save model\n",
    "model_name = f\"BiLSTM_CNN_{optimizer.__class__.__name__}\"\n",
    "\n",
    "model.save(f\"./models/{model_name}.hdf5\")\n",
    "model.save_weights(f\"./models/{model_name}_weights.hdf5\")\n",
    "print(\"Model and model weights saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1NL4Afb102z"
   },
   "source": [
    "From the above log messages, we can observe that the final **f1 score** in test set is **0.855** while its **precision** and **recall** values are **0.851** and **0.859** respectively. We are getting promising evaluation metrics after training the model for just 35 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZHg-Z_i0-5k"
   },
   "source": [
    "Now that training is complete for the 35 epochs that we ran, we can plot a figure to show how the test set and val set f1 score varies with increasing epoch numbers. Ideally, the f1 score for both test and val sets should increase we the number of epochs increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "6bJ6XPqFp_l3",
    "outputId": "0a55b189-6e10-451b-a7a8-2ff91536b371"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "plt.plot(f1_test_history, label = \"test_set\")\n",
    "plt.plot(f1_val_history, label = \"val_set\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"f1_score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu6koZ7h1VK3"
   },
   "source": [
    "We see that with increase in the number of epoches, there is an increase in the f1 score for both test and val sets. This confirms that our model can generalize well to unknown examples. Also, it is important to note that if we increase the number of epochs for which we train the model from 35 to some big number, there is a good chance that the performance of the model will further increase. Howver, due to limitations in computational resources, we have kept the training epochs to 35."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LFeeXDF2YN_"
   },
   "source": [
    "We will save our mapping dictionaries as joblib files so that these can be used later to get the same results. The dictionary mappings might come out to be differnent for each time we train the model. Hence, it is necessary to save the mapping dictionaries that were generated while training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Sj_J8fCeoCcS"
   },
   "outputs": [],
   "source": [
    "for idx_dict, file_name in zip([tag2idx, word2idx, char2idx, case2idx], ['tag2idx.joblib', 'word2idx.joblib', 'char2idx.joblib', 'case2idx.joblib']):\n",
    "    joblib.dump(idx_dict, f'./indexes/{file_name}')\n",
    "\n",
    "print(\"Saved index_dicts as joblib files for future use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgH4thvsq81o"
   },
   "source": [
    "# **Model loading and testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: If we want to load the already trained model, we need to load the index dictionaries that are present in the indexes directory alongwith the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YVZuNFLLlBdY",
    "outputId": "02e12a5b-4993-4e96-924d-59e2a4ca6031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model successfully initialized.\n"
     ]
    }
   ],
   "source": [
    "# We can either load the whole model from our saved trained model\n",
    "model = load_model(f\"./models/BiLSTM_CNN_{optimizer.__class__.__name__}.hdf5\")\n",
    "\n",
    "# OR\n",
    "\n",
    "# We can build the model first and then load the trained weights\n",
    "# model.load_weights(f\"./models/BiLSTM_CNN_{optimizer.__class__.__name__}_weights.hdf5\")\n",
    "\n",
    "print(\"Trained model successfully initialized.\")\n",
    "\n",
    "# We also load the mapping dictionaries.\n",
    "tag2idx = joblib.load(f'./indexes/tag2idx.joblib')\n",
    "word2idx = joblib.load(f'./indexes/word2idx.joblib')\n",
    "char2idx = joblib.load(f'./indexes/char2idx.joblib')\n",
    "case2idx = joblib.load(f'./indexes/case2idx.joblib')\n",
    "\n",
    "idx2tag_ = idx2tag\n",
    "idx2tag = {v:k for k,v in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1vUGj3Gx5z7"
   },
   "source": [
    "We now randomly pick 5 examples from our test set and observe the predictions made by our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           ||True ||Pred\n",
      "==============================\n",
      "stefano        : B-PER B-PER\n",
      "bordon         : I-PER I-PER\n",
      "is             : O     O\n",
      "out            : O     O\n",
      "through        : O     O\n",
      "illness        : O     O\n",
      "and            : O     O\n",
      "coste          : B-PER B-PER\n",
      "said           : O     O\n",
      "he             : O     O\n",
      "had            : O     O\n",
      "dropped        : O     O\n",
      "back           : O     O\n",
      "row            : O     O\n",
      "corrado        : B-PER B-PER\n",
      "UNK            : I-PER I-PER\n",
      ",              : O     O\n",
      "who            : O     O\n",
      "had            : O     O\n",
      "been           : O     O\n",
      "recalled       : O     O\n",
      "for            : O     O\n",
      "the            : O     O\n",
      "england        : B-LOC B-LOC\n",
      "game           : O     O\n",
      "after          : O     O\n",
      "five           : O     O\n",
      "years          : O     O\n",
      "out            : O     O\n",
      "of             : O     O\n",
      "the            : O     O\n",
      "national       : O     O\n",
      "team           : O     O\n",
      ".              : O     O\n"
     ]
    }
   ],
   "source": [
    "i = 27\n",
    "preds, true = predict([test_set[i]])\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, t, pred in zip(test_set[i][0], true[0], preds[0]):\n",
    "    print(\"{:15}: {:5} {}\".format(idx2word[w], idx2tag_[t], idx2tag[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we can clearly see that the model got all the entities correct. It is expected as our model had reached a high score on the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVPKzQtqyQcL",
    "outputId": "e9457b35-d022-4999-b3e4-2bd1f1097e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           ||True ||Pred\n",
      "==============================\n",
      "plo            : B-ORG B-ORG\n",
      "negotiators    : O     O\n",
      "said           : O     O\n",
      "on             : O     O\n",
      "friday         : O     O\n",
      "palestinian    : B-MISC B-MISC\n",
      "president      : O     O\n",
      "yasser         : B-PER B-PER\n",
      "arafat         : I-PER I-PER\n",
      ",              : O     O\n",
      "israeli        : B-MISC B-MISC\n",
      "prime          : O     O\n",
      "minister       : O     O\n",
      "benjamin       : B-PER B-PER\n",
      "netanyahu      : I-PER I-PER\n",
      "and            : O     O\n",
      "egyptian       : B-MISC B-MISC\n",
      "president      : O     O\n",
      "hosni          : B-PER B-PER\n",
      "mubarak        : I-PER I-PER\n",
      "might          : O     O\n",
      "all            : O     O\n",
      "meet           : O     O\n",
      "on             : O     O\n",
      "saturday       : O     O\n",
      "to             : O     O\n",
      "try            : O     O\n",
      "to             : O     O\n",
      "clinch         : O     O\n",
      "a              : O     O\n",
      "deal           : O     O\n",
      "on             : O     O\n",
      "israel         : B-LOC B-LOC\n",
      "'s             : O     O\n",
      "handover       : O     O\n",
      "of             : O     O\n",
      "hebron         : B-LOC B-LOC\n",
      "to             : O     O\n",
      "the            : O     O\n",
      "plo            : B-ORG B-ORG\n",
      ".              : O     O\n"
     ]
    }
   ],
   "source": [
    "i = 1963\n",
    "preds, true = predict([test_set[i]])\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, t, pred in zip(test_set[i][0], true[0], preds[0]):\n",
    "    print(\"{:15}: {:5} {}\".format(idx2word[w], idx2tag_[t], idx2tag[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example as well, the model got all the entities correct. It could correctly identify the name of persons even though it might not have come across those words while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff8MlJFlzJpN",
    "outputId": "eb403db6-4d93-49c0-95f5-e4200c6d4489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           ||True ||Pred\n",
      "==============================\n",
      "rugby          : B-ORG B-ORG\n",
      "union          : I-ORG O\n",
      "-              : O     O\n",
      "little         : B-PER O\n",
      "to             : O     O\n",
      "miss           : O     O\n",
      "campese        : B-PER B-PER\n",
      "farewell       : O     O\n",
      ".              : O     O\n"
     ]
    }
   ],
   "source": [
    "i = 189\n",
    "preds, true = predict([test_set[i]])\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, t, pred in zip(test_set[i][0], true[0], preds[0]):\n",
    "    print(\"{:15}: {:5} {}\".format(idx2word[w], idx2tag_[t], idx2tag[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we see that the model made an error for the word 'union'. The word 'union' is actually a part of the word 'Rugby Union'. However, the model failed to detect this particular entity. This is probably because union is a very generic word and the model failed to identify word based on its actual context use in the sentence. Other than that, the model also got 'little' wrong. In the true labels, it is tagged as a person. However because of the ambiguity around the word 'little', the model could not interpret the word correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hqQ_OJpozP_Q",
    "outputId": "c2200025-de78-4cf5-e414-a736e08ab5a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           ||True ||Pred\n",
      "==============================\n",
      "tasmania       : B-LOC B-LOC\n",
      "352            : O     O\n",
      "for            : O     O\n",
      "three          : O     O\n",
      "(              : O     O\n",
      "david          : B-PER B-PER\n",
      "boon           : I-PER I-PER\n",
      "106            : O     O\n",
      "not            : O     O\n",
      "out            : O     O\n",
      ",              : O     O\n",
      "shaun          : B-PER B-PER\n",
      "young          : I-PER I-PER\n",
      "86             : O     O\n",
      "not            : O     O\n",
      "out            : O     O\n",
      ",              : O     O\n",
      "michael        : B-PER B-PER\n",
      "divenuto       : I-PER I-PER\n",
      "119            : O     O\n",
      ")              : O     O\n",
      "v              : O     O\n",
      "victoria       : B-ORG O\n",
      ".              : O     O\n"
     ]
    }
   ],
   "source": [
    "i = 297\n",
    "preds, true = predict([test_set[i]])\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, t, pred in zip(test_set[i][0], true[0], preds[0]):\n",
    "    print(\"{:15}: {:5} {}\".format(idx2word[w], idx2tag_[t], idx2tag[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the model got all entities correct expect for the word 'victoria' which is actually tagged as an organisation. This is again because the word 'victoria' is not very specific and could easily be taagged as a person in some other sentence as well dependng on the context of the word it is used in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DMiD1oXbzSqo",
    "outputId": "6bdfc336-512d-4ab2-beb2-8d4cba80f39b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           ||True ||Pred\n",
      "==============================\n",
      "many           : O     O\n",
      "of             : O     O\n",
      "these          : O     O\n",
      "citizens       : O     O\n",
      "were           : O     O\n",
      "jews           : B-MISC B-MISC\n",
      "murdered       : O     O\n",
      "during         : O     O\n",
      "the            : O     O\n",
      "war            : O     O\n",
      ",              : O     O\n",
      "when           : O     O\n",
      "nazi           : B-MISC B-MISC\n",
      "german         : B-MISC B-MISC\n",
      "invaders       : O     O\n",
      "killed         : O     O\n",
      "most           : O     O\n",
      "of             : O     O\n",
      "poland         : B-LOC B-LOC\n",
      "'s             : O     O\n",
      "3.5            : O     O\n",
      "million        : O     O\n",
      "jews           : B-MISC B-MISC\n",
      ".              : O     O\n"
     ]
    }
   ],
   "source": [
    "i = 936\n",
    "preds, true = predict([test_set[i]])\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, t, pred in zip(test_set[i][0], true[0], preds[0]):\n",
    "    print(\"{:15}: {:5} {}\".format(idx2word[w], idx2tag_[t], idx2tag[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXe-8LYxzVXb"
   },
   "source": [
    "In the example shown above, the model got all the entities correctly. Words like 'jews', 'nazi', 'german' were correctly identified. it is interesting to note that 'poland' was labelled as a location while 'german' was labelled as misc. So, the model is actually able to differentiate between countries ('poland') and something which is of a country ('german'). If the word 'germany' were to occur somewhere, the model would then have labelled it as a location."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BiLSTM_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
